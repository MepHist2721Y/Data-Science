{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LinearRegression ----> Regression\n",
    "#LogisticRegression ----> Classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#step 1\n",
    "clf = LogisticRegression()\n",
    "clf1 = DecisionTreeClassifier()\n",
    "kn = KNeighborsClassifier()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "irisDataframe = pd.read_csv('data/Iris.csv',names=['Sepal_Length','Sepal_Width', 'Petal_Length', 'Petal_Width','Class'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal_Length</th>\n",
       "      <th>Sepal_Width</th>\n",
       "      <th>Petal_Length</th>\n",
       "      <th>Petal_Width</th>\n",
       "      <th>Class</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "      <td>Iris-setosa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "      <td>Iris-virginica</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Sepal_Length  Sepal_Width  Petal_Length  Petal_Width           Class\n",
       "0             5.1          3.5           1.4          0.2     Iris-setosa\n",
       "1             4.9          3.0           1.4          0.2     Iris-setosa\n",
       "2             4.7          3.2           1.3          0.2     Iris-setosa\n",
       "3             4.6          3.1           1.5          0.2     Iris-setosa\n",
       "4             5.0          3.6           1.4          0.2     Iris-setosa\n",
       "5             5.4          3.9           1.7          0.4     Iris-setosa\n",
       "6             4.6          3.4           1.4          0.3     Iris-setosa\n",
       "7             5.0          3.4           1.5          0.2     Iris-setosa\n",
       "8             4.4          2.9           1.4          0.2     Iris-setosa\n",
       "9             4.9          3.1           1.5          0.1     Iris-setosa\n",
       "10            5.4          3.7           1.5          0.2     Iris-setosa\n",
       "11            4.8          3.4           1.6          0.2     Iris-setosa\n",
       "12            4.8          3.0           1.4          0.1     Iris-setosa\n",
       "13            4.3          3.0           1.1          0.1     Iris-setosa\n",
       "14            5.8          4.0           1.2          0.2     Iris-setosa\n",
       "15            5.7          4.4           1.5          0.4     Iris-setosa\n",
       "16            5.4          3.9           1.3          0.4     Iris-setosa\n",
       "17            5.1          3.5           1.4          0.3     Iris-setosa\n",
       "18            5.7          3.8           1.7          0.3     Iris-setosa\n",
       "19            5.1          3.8           1.5          0.3     Iris-setosa\n",
       "20            5.4          3.4           1.7          0.2     Iris-setosa\n",
       "21            5.1          3.7           1.5          0.4     Iris-setosa\n",
       "22            4.6          3.6           1.0          0.2     Iris-setosa\n",
       "23            5.1          3.3           1.7          0.5     Iris-setosa\n",
       "24            4.8          3.4           1.9          0.2     Iris-setosa\n",
       "25            5.0          3.0           1.6          0.2     Iris-setosa\n",
       "26            5.0          3.4           1.6          0.4     Iris-setosa\n",
       "27            5.2          3.5           1.5          0.2     Iris-setosa\n",
       "28            5.2          3.4           1.4          0.2     Iris-setosa\n",
       "29            4.7          3.2           1.6          0.2     Iris-setosa\n",
       "..            ...          ...           ...          ...             ...\n",
       "120           6.9          3.2           5.7          2.3  Iris-virginica\n",
       "121           5.6          2.8           4.9          2.0  Iris-virginica\n",
       "122           7.7          2.8           6.7          2.0  Iris-virginica\n",
       "123           6.3          2.7           4.9          1.8  Iris-virginica\n",
       "124           6.7          3.3           5.7          2.1  Iris-virginica\n",
       "125           7.2          3.2           6.0          1.8  Iris-virginica\n",
       "126           6.2          2.8           4.8          1.8  Iris-virginica\n",
       "127           6.1          3.0           4.9          1.8  Iris-virginica\n",
       "128           6.4          2.8           5.6          2.1  Iris-virginica\n",
       "129           7.2          3.0           5.8          1.6  Iris-virginica\n",
       "130           7.4          2.8           6.1          1.9  Iris-virginica\n",
       "131           7.9          3.8           6.4          2.0  Iris-virginica\n",
       "132           6.4          2.8           5.6          2.2  Iris-virginica\n",
       "133           6.3          2.8           5.1          1.5  Iris-virginica\n",
       "134           6.1          2.6           5.6          1.4  Iris-virginica\n",
       "135           7.7          3.0           6.1          2.3  Iris-virginica\n",
       "136           6.3          3.4           5.6          2.4  Iris-virginica\n",
       "137           6.4          3.1           5.5          1.8  Iris-virginica\n",
       "138           6.0          3.0           4.8          1.8  Iris-virginica\n",
       "139           6.9          3.1           5.4          2.1  Iris-virginica\n",
       "140           6.7          3.1           5.6          2.4  Iris-virginica\n",
       "141           6.9          3.1           5.1          2.3  Iris-virginica\n",
       "142           5.8          2.7           5.1          1.9  Iris-virginica\n",
       "143           6.8          3.2           5.9          2.3  Iris-virginica\n",
       "144           6.7          3.3           5.7          2.5  Iris-virginica\n",
       "145           6.7          3.0           5.2          2.3  Iris-virginica\n",
       "146           6.3          2.5           5.0          1.9  Iris-virginica\n",
       "147           6.5          3.0           5.2          2.0  Iris-virginica\n",
       "148           6.2          3.4           5.4          2.3  Iris-virginica\n",
       "149           5.9          3.0           5.1          1.8  Iris-virginica\n",
       "\n",
       "[150 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisDataframe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "clf = DecisionTreeClassifier()\n",
    "clf1=KNeighborsClassifier()\n",
    "clf2=LogisticRegression()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris-virginica']\n",
      "['Iris-virginica']\n",
      "['Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "irisarray=irisDataframe.values\n",
    "#row,column\n",
    "X=irisarray[:,0:4]\n",
    "Y=irisarray[:,4]\n",
    "#training the model\n",
    "clf=clf.fit(X,Y)\n",
    "clf1=clf1.fit(X,Y)\n",
    "clf2=clf2.fit(X,Y)\n",
    "\n",
    "#Predicting\n",
    "print(clf.predict([[6.7, 3.1, 5.6, 2.1]]))\n",
    "print(clf1.predict([[6.7, 3.1, 5.6, 2.1]]))\n",
    "print(clf2.predict([[6.7, 3.1, 5.6, 2.1]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisDataframe.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#using dataframe doing splitting manually\n",
    "X_train= irisDataframe.iloc[:120, 0:4]\n",
    "Y_train= irisDataframe.iloc[:120,4:]\n",
    "X_test = irisDataframe.iloc[120:,0:4]\n",
    "Y_test = irisDataframe.iloc[120:,4:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# Split-out validation dataset\n",
    "from sklearn import model_selection\n",
    "#convert into numpy array\n",
    "irisarray = irisDataframe.values\n",
    "\n",
    "X = irisarray[:,0:4]\n",
    "Y = irisarray[:,4]\n",
    "print(type(Y))\n",
    "validation_size = 0.20\n",
    "seed =8\n",
    "X_train, X_validation, Y_train, Y_validation = model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 4)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_validation.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.843333333333335\n",
      "3.0540000000000007\n",
      "3.7586666666666693\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(irisDataframe [\"Sepal_Length\"].mean())\n",
    "print(irisDataframe [\"Sepal_Width\"].mean())\n",
    "print(irisDataframe [\"Petal_Length\"].mean())\n",
    "m= irisDataframe [\"Petal_Width\"].mean()\n",
    "type(irisDataframe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "#irisDataframe.isnull()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.1986666666666672\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0      0.2\n",
       "1      0.2\n",
       "2      0.2\n",
       "3      0.2\n",
       "4      0.2\n",
       "5      0.4\n",
       "6      0.3\n",
       "7      0.2\n",
       "8      0.2\n",
       "9      0.1\n",
       "10     0.2\n",
       "11     0.2\n",
       "12     0.1\n",
       "13     0.1\n",
       "14     0.2\n",
       "15     0.4\n",
       "16     0.4\n",
       "17     0.3\n",
       "18     0.3\n",
       "19     0.3\n",
       "20     0.2\n",
       "21     0.4\n",
       "22     0.2\n",
       "23     0.5\n",
       "24     0.2\n",
       "25     0.2\n",
       "26     0.4\n",
       "27     0.2\n",
       "28     0.2\n",
       "29     0.2\n",
       "      ... \n",
       "120    2.3\n",
       "121    2.0\n",
       "122    2.0\n",
       "123    1.8\n",
       "124    2.1\n",
       "125    1.8\n",
       "126    1.8\n",
       "127    1.8\n",
       "128    2.1\n",
       "129    1.6\n",
       "130    1.9\n",
       "131    2.0\n",
       "132    2.2\n",
       "133    1.5\n",
       "134    1.4\n",
       "135    2.3\n",
       "136    2.4\n",
       "137    1.8\n",
       "138    1.8\n",
       "139    2.1\n",
       "140    2.4\n",
       "141    2.3\n",
       "142    1.9\n",
       "143    2.3\n",
       "144    2.5\n",
       "145    2.3\n",
       "146    1.9\n",
       "147    2.0\n",
       "148    2.3\n",
       "149    1.8\n",
       "Name: Petal_Width, Length: 150, dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Imputation\n",
    "print(m)\n",
    "irisDataframe[\"Petal_Width\"].fillna(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "irisDataframe.isnull().values.any()\n",
    "mean_value=irisDataframe['Petal_Width'].mean()\n",
    "irisDataframe['Petal_Width'].fillna(value=irisDataframe['Petal_Width'].mean(), inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Iris-virginica     50\n",
       "Iris-versicolor    50\n",
       "Iris-setosa        50\n",
       "Name: Class, dtype: int64"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#peep into the dataset\n",
    "\n",
    "import sklearn\n",
    "from sklearn import model_selection\n",
    "irisDataframe\n",
    "irisDataframe['Class'].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Statistical Summary of the dataset\n",
    "#irisDataframe.describe()\n",
    "irisDataframe\n",
    "g=irisDataframe.groupby('Class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Iris-setosa': Int64Index([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16,\n",
       "             17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33,\n",
       "             34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49],\n",
       "            dtype='int64'),\n",
       " 'Iris-versicolor': Int64Index([50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66,\n",
       "             67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79, 80, 81, 82, 83,\n",
       "             84, 85, 86, 87, 88, 89, 90, 91, 92, 93, 94, 95, 96, 97, 98, 99],\n",
       "            dtype='int64'),\n",
       " 'Iris-virginica': Int64Index([100, 101, 102, 103, 104, 105, 106, 107, 108, 109, 110, 111, 112,\n",
       "             113, 114, 115, 116, 117, 118, 119, 120, 121, 122, 123, 124, 125,\n",
       "             126, 127, 128, 129, 130, 131, 132, 133, 134, 135, 136, 137, 138,\n",
       "             139, 140, 141, 142, 143, 144, 145, 146, 147, 148, 149],\n",
       "            dtype='int64')}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check number of instances/rows in each class\n",
    "#classwise distribution of rows\n",
    "g=irisDataframe.groupby('Class')\n",
    "g.groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sepal_Length</th>\n",
       "      <th>Sepal_Width</th>\n",
       "      <th>Petal_Length</th>\n",
       "      <th>Petal_Width</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.054000</td>\n",
       "      <td>3.758667</td>\n",
       "      <td>1.198667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.433594</td>\n",
       "      <td>1.764420</td>\n",
       "      <td>0.763161</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Sepal_Length  Sepal_Width  Petal_Length  Petal_Width\n",
       "count    150.000000   150.000000    150.000000   150.000000\n",
       "mean       5.843333     3.054000      3.758667     1.198667\n",
       "std        0.828066     0.433594      1.764420     0.763161\n",
       "min        4.300000     2.000000      1.000000     0.100000\n",
       "25%        5.100000     2.800000      1.600000     0.300000\n",
       "50%        5.800000     3.000000      4.350000     1.300000\n",
       "75%        6.400000     3.300000      5.100000     1.800000\n",
       "max        7.900000     4.400000      6.900000     2.500000"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "irisDataframe.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Iris-virginica'], dtype=object)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import model_selection\n",
    "irisarray=irisDataframe.values\n",
    "\n",
    "X=irisarray[:,0:4]\n",
    "Y=irisarray[:,4]\n",
    "#training the model\n",
    "clf=clf.fit(X,Y)\n",
    "\n",
    "#Predicting\n",
    "clf.predict([[6.5, 3.2, 5.1, 2.0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "irisDataframe.shape= (150, 5)  irisDataframe.size = 750\n",
      "X_train.shape      = (120, 4)  X_train.size = 480\n",
      "X_validation.shape = (30, 4)   X_validation.size = 120\n",
      "Y_train.shape      = (120,)    Y_train.size = 120\n",
      "Y_validation.shape = (30,)     Y_validation.size = 30\n"
     ]
    }
   ],
   "source": [
    "print (\"irisDataframe.shape=\",irisDataframe.shape, \" irisDataframe.size =\", irisDataframe.size)\n",
    "print (\"X_train.shape      =\",X_train.shape, \" X_train.size =\", X_train.size)\n",
    "print (\"X_validation.shape =\",X_validation.shape, \"  X_validation.size =\", X_validation.size)\n",
    "print (\"Y_train.shape      =\",Y_train.shape, \"   Y_train.size =\", Y_train.size)\n",
    "print (\"Y_validation.shape =\",Y_validation.shape, \"    Y_validation.size =\", Y_validation.size)\n",
    "#print (\"X Train data =\",X_train)\n",
    "#print(\"x validation data=\", X_validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "(150, 4)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "# Split-out validation dataset\n",
    "\n",
    "from sklearn import model_selection\n",
    "#Return a Numpy representation of the DataFrame...so irisarray is a numpy.ndarray\n",
    "# only values are returned, the axes labels will be removed.\n",
    "irisarray = irisDataframe.values\n",
    "X = irisarray[:,0:4]\n",
    "print(type(X))\n",
    "print(X.shape)\n",
    "Y = irisarray[:,4]\n",
    "print(Y.shape)\n",
    "validation_size = 0.05\n",
    "seed = 9\n",
    "X_train, X_test, Y_train, Y_test =model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9666666666666667\n",
      "[[ 0  0]\n",
      " [ 1 29]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1030: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  if pos_label not in present_labels:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "pos_label=1 is not a valid label: array(['Iris-versicolor', 'Iris-virginica'], dtype='<U15')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-115-3890d986b7bc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrecall_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mrecall_score\u001b[1;34m(y_true, y_pred, labels, pos_label, average, sample_weight)\u001b[0m\n\u001b[0;32m   1357\u001b[0m                                                  \u001b[0maverage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maverage\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1358\u001b[0m                                                  \u001b[0mwarn_for\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'recall'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1359\u001b[1;33m                                                  sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m   1360\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mr\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1361\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py\u001b[0m in \u001b[0;36mprecision_recall_fscore_support\u001b[1;34m(y_true, y_pred, beta, labels, pos_label, average, warn_for, sample_weight)\u001b[0m\n\u001b[0;32m   1034\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1035\u001b[0m                     raise ValueError(\"pos_label=%r is not a valid label: %r\" %\n\u001b[1;32m-> 1036\u001b[1;33m                                      (pos_label, present_labels))\n\u001b[0m\u001b[0;32m   1037\u001b[0m             \u001b[0mlabels\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mpos_label\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1038\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: pos_label=1 is not a valid label: array(['Iris-versicolor', 'Iris-virginica'], dtype='<U15')"
     ]
    }
   ],
   "source": [
    "#metrics\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "\n",
    "predictions = clf2.predict(X_test)\n",
    "predictions\n",
    "a=accuracy_score(predictions, Y_test)\n",
    "c=confusion_matrix(Y_test, predictions)\n",
    "print(a)\n",
    "print(c)\n",
    "print(classification_report(Y_test, predictions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Iris-setosa' 'Iris-setosa' 'Iris-setosa' 'Iris-virginica'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-setosa' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-versicolor' 'Iris-versicolor' 'Iris-setosa'\n",
      " 'Iris-virginica' 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-virginica' 'Iris-virginica' 'Iris-virginica' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-versicolor' 'Iris-versicolor'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-virginica' 'Iris-setosa'\n",
      " 'Iris-setosa' 'Iris-virginica']\n",
      "The accuracy is  0.9333333333333333\n",
      "[[10  0  0]\n",
      " [ 0  8  1]\n",
      " [ 0  1 10]]\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        10\n",
      "Iris-versicolor       0.89      0.89      0.89         9\n",
      " Iris-virginica       0.91      0.91      0.91        11\n",
      "\n",
      "    avg / total       0.93      0.93      0.93        30\n",
      "\n",
      "Recall score: 0.9333333333333333\n",
      "Precision score: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "#LinearRegression ----> Regression\n",
    "#LogisticRegression ----> Classification\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import precision_score\n",
    "#step 1\n",
    "clf = LogisticRegression()\n",
    "\n",
    "clf = clf.fit(X_train,Y_train)\n",
    "#clf.predict([[0.2,0,.2,.2]])\n",
    "\n",
    "pred = clf.predict(X_validation)\n",
    "print(pred)\n",
    "acc=accuracy_score(pred, Y_validation)\n",
    "print(\"The accuracy is \", acc)\n",
    "c=confusion_matrix(Y_validation, pred)\n",
    "print(c)\n",
    "print(classification_report(Y_validation, pred))\n",
    "print(\"Recall score:\",recall_score(pred,Y_validation, average='micro'))\n",
    "print(\"Precision score:\",precision_score(pred,Y_validation, average='micro'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "LogisticRegression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import svm, datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import pyplot\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "plot_confusion_matrix(clf, X_validation, y_validation)  \n",
    "plt.show() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Iris-setosa', 'Iris-virginica', 'Iris-versicolor', 'Iris-setosa',\n",
    "       'Iris-virginica', 'Iris-virginica', 'Iris-virginica',\n",
    "       'Iris-versicolor'\n",
    "        \n",
    "        \n",
    "#actual\n",
    "rray(['Iris-setosa', 'Iris-versicolor', 'Iris-versicolor', 'Iris-setosa',\n",
    "       'Iris-virginica', 'Iris-virginica', 'Iris-virginica',\n",
    "       'Iris-versicolor'], dtype=object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "['Iris-virginica' 'Iris-versicolor' 'Iris-virginica' 'Iris-virginica'\n",
      " 'Iris-versicolor' 'Iris-setosa' 'Iris-setosa' 'Iris-setosa']\n",
      "********Confusion Matrix********\n",
      "[[3 0 0]\n",
      " [0 2 0]\n",
      " [0 0 3]]\n",
      "Classification Report\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'                 precision    recall  f1-score   support\\n\\n    Iris-setosa       1.00      1.00      1.00         3\\nIris-versicolor       1.00      1.00      1.00         2\\n Iris-virginica       1.00      1.00      1.00         3\\n\\n    avg / total       1.00      1.00      1.00         8\\n'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LinearRegression ----> Regression\n",
    "#LogisticRegression ----> Classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "#step 1\n",
    "clf = LogisticRegression()\n",
    "clf = clf.fit(X_train,Y_train)\n",
    "#print(clf.predict([[4,2,3,4]]))\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "dt= DecisionTreeClassifier()\n",
    "dt= dt.fit(X_train,Y_train)\n",
    "\n",
    "clf.predict([[3.2,1,4,5]])\n",
    "\n",
    "\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "print(accuracy_score(Y_test, predictions))\n",
    "print(predictions)\n",
    "\n",
    "c=confusion_matrix(Y_test, predictions)\n",
    "print\n",
    "print(\"********Confusion Matrix********\")\n",
    "print(c)\n",
    "print\n",
    "print(\"Classification Report\")\n",
    "classification_report(Y_test, predictions)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mC:\\Users\\404ERR~1\\AppData\\Local\\Temp/ipykernel_9280/2412532134.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m#models = ('LR',  LogisticRegression()\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecisionTreeClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;31m#evaluate model  #to make five partitions of all training data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m#each split will be 30 records if five splits\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "#Logistic regression is classification\n",
    "from sklearn import model_selection\n",
    "\n",
    "# Test options and evaluation metric\n",
    "seed = 7\n",
    "scoring = 'accuracy'\n",
    "# Spot Check Algorithms\n",
    "#models = ('LR',  LogisticRegression()\n",
    "clf = DecisionTreeClassifier()\n",
    "clf = clf.fit(X_train,Y_train)\n",
    "#evaluate model  #to make five partitions of all training data\n",
    "#each split will be 30 records if five splits\n",
    "kfold = model_selection.KFold(n_splits=5, random_state=seed)\n",
    "#150 records   5 splits  30 each\n",
    "cv_results = model_selection.cross_val_score( LogisticRegression(), X_train, Y_train, cv=kfold)\n",
    "print(cv_results)\n",
    "print(len(cv_results))\n",
    "print(type(cv_results))\n",
    "msg = \"%s: %f (%f)\" % (\"Logistic Regression\", cv_results.mean(), cv_results.std())\n",
    "#msg = \"%s: %f (%f)\" % (name, cv_results, cv_results)\n",
    "print(msg)\n",
    "kfold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "y_pred = [0, 2, 1, 3,4]\n",
    "y_true = [0, 1, 2, 3,4]\n",
    "accuracy_score(y_true, y_pred)\n",
    "\n",
    "accuracy_score(y_true, y_pred, normalize=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('LR', LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
      "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
      "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
      "          verbose=0, warm_start=False)), ('LDA', LinearDiscriminantAnalysis(n_components=None, priors=None, shrinkage=None,\n",
      "              solver='svd', store_covariance=False, tol=0.0001)), ('KNN', KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
      "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
      "           weights='uniform')), ('CART', DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')), ('NB', GaussianNB(priors=None)), ('SVM', SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
      "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
      "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
      "  tol=0.001, verbose=False))]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Test options and evaluation metric\n",
    "seed = 7\n",
    "#the seed ensures we have the same sequence of random numbers. \n",
    "#The random numbers ensure we have a random split of the data into the k folds.\n",
    "scoring = 'accuracy'\n",
    "# Spot Check Algorithms\n",
    "models = []\n",
    "models.append(('LR',  LogisticRegression()))\n",
    "models.append(('LDA', LinearDiscriminantAnalysis()))\n",
    "models.append(('KNN', KNeighborsClassifier(n_neighbors=5)))\n",
    "models.append(('CART', DecisionTreeClassifier()))\n",
    "models.append(('NB', GaussianNB()))\n",
    "models.append(('SVM', SVC()))\n",
    "\n",
    "print(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR : 0.941667 (0.062361)\n",
      "LDA : 0.975000 (0.050000)\n",
      "KNN : 0.958333 (0.058926)\n",
      "CART : 0.925000 (0.061237)\n",
      "NB : 0.941667 (0.077280)\n",
      "SVM : 0.950000 (0.061237)\n",
      "[array([1.   , 1.   , 0.875, 0.875, 0.875, 0.875, 0.875, 0.875, 1.   ,\n",
      "       1.   , 1.   , 1.   , 0.875, 1.   , 1.   ]), array([1.   , 1.   , 0.875, 0.875, 1.   , 0.875, 1.   , 1.   , 1.   ,\n",
      "       1.   , 1.   , 1.   , 1.   , 1.   , 1.   ]), array([1.   , 1.   , 0.875, 1.   , 1.   , 0.875, 0.875, 1.   , 0.875,\n",
      "       1.   , 1.   , 1.   , 0.875, 1.   , 1.   ]), array([0.875, 1.   , 0.875, 0.875, 0.875, 0.875, 0.875, 1.   , 0.875,\n",
      "       1.   , 1.   , 0.875, 0.875, 1.   , 1.   ]), array([1.   , 1.   , 0.875, 0.75 , 1.   , 1.   , 0.875, 1.   , 1.   ,\n",
      "       1.   , 0.875, 0.875, 0.875, 1.   , 1.   ]), array([0.875, 1.   , 1.   , 0.875, 1.   , 0.875, 0.875, 1.   , 1.   ,\n",
      "       1.   , 1.   , 0.875, 1.   , 1.   , 0.875])]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\discriminant_analysis.py:442: UserWarning: The priors do not sum to 1. Renormalizing\n",
      "  UserWarning)\n"
     ]
    }
   ],
   "source": [
    "results=[]\n",
    "names=[]\n",
    "#putting a loop for testing various models\n",
    "for name, model in models:\n",
    "    kfold=model_selection.KFold(n_splits=15, random_state=seed)\n",
    "    cv_results = model_selection.cross_val_score(model,X_train,Y_train, cv=kfold)\n",
    "    results.append(cv_results)\n",
    "    names.append(name)\n",
    "    msg = \"%s : %f (%f)\"%(name,cv_results.mean(),cv_results.std())\n",
    "    print(msg)\n",
    "    \n",
    "print(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model_selection.cross_val_score(model, X, Y, cv=15)\n",
    "scores\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#0 or 1\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Make predictions on validation dataset\n",
    "knn = KNeighborsClassifier(n_neighbors=15, n_jobs=3)\n",
    "\n",
    "knn.fit(X_train, Y_train)\n",
    "#knn internal parameters have been set\n",
    "predictions = knn.predict(X_validation)\n",
    "print(predictions)\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####Metrics for model selection\n",
    "print(\"Accuracy Score :\", accuracy_score( predictions, Y_validation))\n",
    "print(\"Confusion Matrix : \\n\",confusion_matrix( predictions, Y_validation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(\"Classification Report :\\n\",classification_report(Y_validation, predictions))\n",
    "#P =  [[5,3,1,.2]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P =[[6,3,7,4]]\n",
    "print (\"4) Using KNN Prediction is \" + str(knn.predict(P)) +\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for Iris-setosa\n",
    "irisDataframe[irisDataframe['class']=='Iris-setosa'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now we will see in the following cells parameter tuning using cross_val_score\n",
    "\n",
    "#Goal: Select the best tuning parameters or \"hyperparameters\" for KNN on the iris dataset\n",
    "\n",
    "#To select the best value of k for KNN model to predict species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports \n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X matrix dimensionality: (150, 4)\n",
      "Y vector dimensionality: (150,)\n"
     ]
    }
   ],
   "source": [
    "# read in the iris data\n",
    "iris = load_iris()\n",
    "\n",
    "# create X (features) and y (response)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "print('X matrix dimensionality:', X.shape)\n",
    "print('Y vector dimensionality:', y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.         0.93333333 1.         1.         0.86666667 0.93333333\n",
      " 0.93333333 1.         1.         1.        ]\n"
     ]
    }
   ],
   "source": [
    "# 10-fold (cv=10) cross-validation with K=5 (n_neighbors=5) for KNN (the n_neighbors parameter)\n",
    "\n",
    "# instantiate model\n",
    "knn = KNeighborsClassifier(n_neighbors=5)\n",
    "\n",
    "# store scores in scores object\n",
    "# scoring metric used here is 'accuracy' because it's a classification problem\n",
    "# cross_val_score takes care of splitting X and y into the 10 folds that's why we pass X and y entirely \n",
    "#instead of X_train and y_train\n",
    "scores = cross_val_score(knn, X, y, cv=10, scoring='accuracy')\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use average accuracy as an estimate of out-of-sample accuracy\n",
    "\n",
    "# scores is a numpy array so we can use   the mean method\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.9591049382716049, 0.9390432098765431, 0.9660493827160493, 0.9660493827160493, 0.9660493827160493, 0.9729938271604938, 0.9729938271604938, 0.9591049382716049, 0.9660493827160493, 0.9729938271604938, 0.98070987654321, 0.9737654320987654, 0.9737654320987654, 0.9737654320987654, 0.9737654320987654, 0.9737654320987654, 0.9737654320987654, 0.9737654320987654, 0.9668209876543209, 0.9737654320987654, 0.9737654320987654, 0.9668209876543209, 0.9598765432098766, 0.9529320987654321, 0.9598765432098766, 0.9529320987654321, 0.9537037037037037, 0.9398148148148149, 0.9537037037037037, 0.9467592592592592, 0.9467592592592592, 0.9390432098765432, 0.9459876543209877, 0.9529320987654321, 0.9529320987654321, 0.9459876543209877, 0.9398148148148149, 0.9459876543209877, 0.9398148148148149, 0.933641975308642, 0.933641975308642, 0.933641975308642, 0.933641975308642, 0.9266975308641975, 0.9405864197530863, 0.9336419753086419, 0.9336419753086419, 0.9197530864197531, 0.9336419753086419, 0.9197530864197531, 0.9336419753086419, 0.9197530864197531, 0.9266975308641975, 0.9128086419753086, 0.9128086419753086, 0.9058641975308642, 0.9058641975308642, 0.8927469135802469, 0.8996913580246915, 0.8858024691358025, 0.8858024691358025, 0.8858024691358025, 0.8919753086419752, 0.8796296296296297, 0.8935185185185185, 0.8796296296296297, 0.886574074074074, 0.886574074074074, 0.886574074074074, 0.8796296296296297, 0.8796296296296297, 0.8796296296296297, 0.8796296296296297, 0.8927469135802468, 0.8858024691358025, 0.8989197530864197, 0.8865740740740741, 0.8865740740740741, 0.8796296296296297]\n"
     ]
    }
   ],
   "source": [
    "# search for an optimal value of K for KNN\n",
    "\n",
    "# list of integers 1 to 30\n",
    "# integers we want to try\n",
    "k_range = range(1, 80)\n",
    "# list of scores from k_range\n",
    "k_scores = []\n",
    "\n",
    "# 1. we will loop through reasonable values of k\n",
    "for k in k_range:\n",
    "    # 2. run KNeighborsClassifier with k neighbours and obtain cross_val_score for KNeighborsClassifier with k neighbours\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    scores = cross_val_score(knn, X, y, cv=6)\n",
    "    # 4. append mean of scores for k neighbors to k_scores list\n",
    "    k_scores.append(scores.mean())\n",
    "print(k_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0,0.5,'Cross-Validated Accuracy')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x21051de4c18>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot the value of K for KNN (x-axis) versus the cross-validated accuracy (y-axis)\n",
    "plt.plot(k_range, k_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#More efficient parameter tuning using GridSearchCV\n",
    "#Allows you to define a grid of parameters that will be searched using K-fold cross-validation\n",
    "\n",
    "#This is like an automated version of the \"for loop\" above\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "# define the parameter values that should be searched\n",
    "\n",
    "k_range = list(range(1, 31))\n",
    "print(k_range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a parameter grid: map the parameter names to the values that should be searched\n",
    "# simply a python dictionary\n",
    "# key: parameter name\n",
    "# value: list of values that should be searched for that parameter\n",
    "# single key-value pair for param_grid\n",
    "param_grid = dict(n_neighbors=k_range)\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate the grid\n",
    "grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Grid object is ready to do 10-fold cross validation on a KNN model using classification accuracy as the evaluation metric\n",
    "#In addition, there is a parameter grid to repeat the 10-fold cross validation process 30 times\n",
    "#Each time, the n_neighbors parameter should be given a different value from the list\n",
    "#We can't give GridSearchCV just a list\n",
    "#We've to specify n_neighbors should take on 1 through 30\n",
    "#You can set n_jobs = -1 to run computations in parallel (if supported by your computer and OS)\n",
    "#This is also called parallel programming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit the grid with data\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Remember this is running 10-fold validation 30 times\n",
    "\n",
    "#KNN model is being fit and predictions are being made 30 x 10 = 300 times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the complete results (list of named tuples)\n",
    "grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#List of 30 named tuples\n",
    "\n",
    "#First tuple\n",
    "#When n_neighbors = 1\n",
    "#Mean of accuracy scores = 0.96\n",
    "#Standard deviation of accuracy scores = 0.053\n",
    "#If SD is high, the cross-validated estimate of the accuracy might not be as reliable\n",
    "#There is one tuple for each of the 30 trials of CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the first tuple\n",
    "# we will slice the list and select its elements using dot notation and []\n",
    "\n",
    "\n",
    "print('Parameters')\n",
    "print(grid.grid_scores_[0].parameters)\n",
    "print(grid.grid_scores_[1].parameters)\n",
    "print(grid.grid_scores_[2].parameters)\n",
    "\n",
    "# Array of 10 accuracy scores during 10-fold cv using the parameters\n",
    "print('')\n",
    "print('CV Validation Score')\n",
    "print(grid.grid_scores_[0].cv_validation_scores)\n",
    "\n",
    "# Mean of the 10 scores\n",
    "print('')\n",
    "print('Mean Validation Score')\n",
    "print(grid.grid_scores_[0].mean_validation_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a list of the mean scores only\n",
    "# list comprehension to loop through grid.grid_scores\n",
    "grid_mean_scores = [result.mean_validation_score for result in grid.grid_scores_]\n",
    "print(grid_mean_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "# this is identical to the one we generated above\n",
    "plt.plot(k_range, grid_mean_scores)\n",
    "plt.xlabel('Value of K for KNN')\n",
    "plt.ylabel('Cross-Validated Accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the best model\n",
    "\n",
    "# Single best score achieved across all params (k)\n",
    "print(grid.best_score_)\n",
    "\n",
    "# Dictionary containing the parameters (k) used to generate that score\n",
    "print(grid.best_params_)\n",
    "\n",
    "# Actual model object fit with those best parameters\n",
    "# Shows default parameters that we did not specify\n",
    "print(grid.best_estimator_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Searching multiple parameters simultaneously\n",
    "#Example: tuning max_depth and min_samples_leaf for a DecisionTreeClassifier\n",
    "#Could tune parameters independently: change max_depth while leaving min_samples_leaf at its default value, and vice versa\n",
    "#But, best performance might be achieved when neither parameter is at its default value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the parameter values that should be searched\n",
    "k_range = list(range(1, 31))\n",
    "\n",
    "# Another parameter besides k that we might vary is the weights parameters\n",
    "# default options --> uniform (all points in the neighborhood are weighted equally)\n",
    "# another option --> distance (weights closer neighbors more heavily than further neighbors)\n",
    "\n",
    "# we create a list\n",
    "weight_options = ['uniform', 'distance']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a parameter grid: map the parameter names to the values that should be searched\n",
    "# dictionary = dict(key=values, key=values)\n",
    "param_grid = dict(n_neighbors=k_range, weights=weight_options)\n",
    "print(param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate and fit the grid\n",
    "# exhaustive grid-search because it's trying every combination\n",
    "# 10-fold cross-validation is being performed 30 x 2 = 60 times\n",
    "\n",
    "grid = GridSearchCV(knn, param_grid, cv=10, scoring='accuracy')\n",
    "grid.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the complete results\n",
    "grid.grid_scores_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# examine the best model\n",
    "print(grid.best_score_)\n",
    "print(grid.best_params_)\n",
    "\n",
    "# Best score did not improve for this model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "def plot_learning_curves(model, X, y):\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    train_errors, val_errors = [], []\n",
    "    for m in range(1, len(X_train)):\n",
    "        model.fit(X_train[:m], y_train[:m])\n",
    "        y_train_predict = model.predict(X_train[:m])\n",
    "        y_val_predict = model.predict(X_val)\n",
    "        train_errors.append(mean_squared_error(y_train[:m], y_train_predict))\n",
    "        val_errors.append(mean_squared_error(y_val, y_val_predict))\n",
    "    plt.plot(np.sqrt(train_errors), \"r-+\", linewidth=2, label=\"train\")\n",
    "    plt.plot(np.sqrt(val_errors), \"b-\", linewidth=3, label=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "import matplotlib.pyplot as plt\n",
    "knn = KNeighborsClassifier(n_neighbors=1)\n",
    "# read in the iris data\n",
    "iris = load_iris()\n",
    "\n",
    "# create X (features) and y (response)\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "plot_learning_curves(knn,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the data attributes for the Iris dataset.\n",
    "\n",
    "from sklearn import preprocessing\n",
    "# load the iris dataset\n",
    "\n",
    "print(iris.data.shape)\n",
    "# Split-out validation dataset\n",
    "from sklearn import model_selection\n",
    "#Return a Numpy representation of the DataFrame...so irisarray is a numpy.ndarray\n",
    "# only values are returned, the axes labels will be removed.\n",
    "irisarray = irisDataframe.values\n",
    "X = irisarray[:,0:4]\n",
    "print(type(X))\n",
    "print(X.shape)\n",
    "Y = irisarray[:,4]\n",
    "print(Y.shape)\n",
    "validation_size = 0.20\n",
    "seed = 7\n",
    "X_train, X_validation, Y_train, Y_validation =model_selection.train_test_split(X, Y, test_size=validation_size, random_state=seed)\n",
    "# separate the data from the target attributes\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0:5])\n",
    "      # normalize the data attributes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_X = preprocessing.normalize(X_train)\n",
    "normalized_X[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "# define data\n",
    "data = asarray([[100, 0.001],\n",
    "[8, 0.05],\n",
    "[50, 0.005],\n",
    "[88, 0.07],\n",
    "[4, 0.1]])\n",
    "print(data)\n",
    "# define min max scaler\n",
    "scaler = MinMaxScaler()\n",
    "# transform data\n",
    "scaled = scaler.fit_transform(data)\n",
    "print(scaled)\n",
    "# convert the array back to a dataframe\n",
    "dataset = pd.DataFrame(data)\n",
    "# summarize\n",
    "print(dataset.describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# define min max scaler\n",
    "scaler = MinMaxScaler()\n",
    "# transform data\n",
    "scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "# convert the array back to a dataframe\n",
    "dataset = pd.DataFrame(scaled)\n",
    "\n",
    "# summarize\n",
    "\n",
    "print(dataset.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize the data attributes for the Iris dataset.\n",
    "\n",
    "from sklearn import preprocessing\n",
    "\n",
    "# standardize the data attributes\n",
    "standardized_X = preprocessing.scale(X_train)\n",
    "#standardized_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# define data\n",
    "data = asarray([[100, 0.001],\n",
    "[8, 0.05],\n",
    "[50, 0.005],\n",
    "[88, 0.07],\n",
    "[4, 0.1]])\n",
    "print(data)\n",
    "# define standard scaler\n",
    "scaler = StandardScaler()\n",
    "# transform data\n",
    "scaled = scaler.fit_transform(data)\n",
    "print(scaled)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# define data\n",
    "data = X_train\n",
    "print(data)\n",
    "# define standard scaler\n",
    "scaler = StandardScaler()\n",
    "# transform data\n",
    "scaled = scaler.fit_transform(data)\n",
    "print(scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import asarray\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# define min max scaler\n",
    "scaler = StandardScaler()\n",
    "# transform data\n",
    "scaled = scaler.fit_transform(X_train)\n",
    "# convert the array back to a dataframe\n",
    "dataset = pd.DataFrame(X_train)\n",
    "# summarize\n",
    "print(dataset.describe())\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate knn on the sonar dataset with standard scaler transform\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas import read_csv\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from matplotlib import pyplot\n",
    "# load dataset\n",
    "url = \"https://raw.githubusercontent.com/jbrownlee/Datasets/master/sonar.csv\"\n",
    "dataset = read_csv(url, header=None)\n",
    "data = dataset.values\n",
    "# separate into input and output columns\n",
    "X, y = data[:, :-1], data[:, -1]\n",
    "# ensure inputs are floats and output is an integer label\n",
    "X = X.astype('float32')\n",
    "y = LabelEncoder().fit_transform(y.astype('str'))\n",
    "# define the pipeline\n",
    "trans = StandardScaler()\n",
    "model = KNeighborsClassifier()\n",
    "pipeline = Pipeline(steps=[('t', trans), ('m', model)])\n",
    "# evaluate the pipeline\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(pipeline, X, y, scoring='accuracy', cv=cv, n_jobs=-1)\n",
    "# report pipeline performance\n",
    "print('Accuracy: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
